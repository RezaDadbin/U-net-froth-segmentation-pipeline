# ðŸ«§ UNet Froth Segmentation Pipeline

A modular **PyTorch** implementation of **UNet** for industrial froth image segmentation. This repository follows the **same workflow style** as the SBSâ€‘Net froth pipeline: central configuration file, and four topâ€‘level scripts for **train**, **evaluate**, **predict**, and **postâ€‘process**.

**Authors:** [Reza Dadbin](https://github.com/RezaDadbin) Â· [Sina Lotfi](https://github.com/cinaLotfi)

---

## Table of contents

- [Key features](#key-features)
- [Repository structure](#repository-structure)
- [Getting started](#getting-started)
- [Prepare your dataset](#prepare-your-dataset)
- [Configure the pipeline](#configure-the-pipeline)
- [Run the training & evaluation workflow](#run-the-training--evaluation-workflow)
  - [1. Train](#1-train)
  - [2. Evaluate](#2-evaluate)
  - [3. Predict soft masks](#3-predict-soft-masks)
  - [4. Watershed postâ€‘processing](#4-watershed-post-processing)
- [Outputs](#outputs)
- [Reproducibility tips](#reproducibility-tips)
- [Extending the project](#extending-the-project)
- [Troubleshooting](#troubleshooting)
- [Authors & citation](#authors--citation)

---

## Key features

- **UNet** backbone implemented in `unet_froth/models/unet.py`.
- **scripts/** for the full workflow: training, evaluation, prediction of probability maps, and **watershed** postâ€‘processing.
- **Polygon JSON** labels (LabelMe/simple/COCOâ€‘like) automatically rasterized into binary masks.
- **Single `config.py`** holds paths & hyperparameters (mirrors the SBSâ€‘Net style).
- Inference is **dtypeâ€‘safe** and resizes probability maps back to the **original image size** for saving.
- Designed to be **clear, reproducible, and productionâ€‘friendly**.

> This pipelineâ€™s **structure** is inspired by the SBSâ€‘Net froth segmentation repo; we adapt the same ideas to a UNet implementation.

---

## Repository structure

```
UNet-froth-segmentation-pipeline/
â”œâ”€â”€ README.md
â”œâ”€â”€ config.py                 # global configuration used by all scripts
â”œâ”€â”€ unet_froth/
â”‚   â”œâ”€â”€ models/               # UNet architecture
â”‚   â””â”€â”€ utils/                # metrics, common helpers, visualize, postprocess
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py              # training loop (saves best checkpoint)
â”‚   â”œâ”€â”€ eval.py               # compute IoU/Dice on eval split
â”‚   â”œâ”€â”€ predict.py            # export soft probability maps as PNGs
â”‚   â””â”€â”€ postprocess.py        # watershed instance refinement -> binary masks
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train/                # paired images + sameâ€‘stem JSON polygons
â”‚   â”œâ”€â”€ val/
â”‚   â””â”€â”€ eval/
â”œâ”€â”€ checkpoints/
â””â”€â”€ outputs/
    â”œâ”€â”€ pred_masks/
    â””â”€â”€ postprocessed_masks/
```

---

## Getting started

> Python **3.11+** recommended. On macOS, if OpenCV complains, run `xcode-select --install`.

```bash
python -m venv .venv
source .venv/bin/activate            # Windows: .venv\Scripts\activate
python -m pip install --upgrade pip
pip install -r requirements.txt
```

---

## Prepare your dataset

Place your images and annotations in `data/`:

```
data/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ img_0001.tiff
â”‚   â”œâ”€â”€ img_0001.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ val/
â”‚   â”œâ”€â”€ img_0101.png
â”‚   â”œâ”€â”€ img_0101.json
â”‚   â””â”€â”€ ...
â””â”€â”€ eval/
    â”œâ”€â”€ img_0201.jpg
    â”œâ”€â”€ img_0201.json      # optional; present if you want metrics on eval
    â””â”€â”€ ...
```

- Supported image formats: `.tiff/.tif/.png/.jpg` (RGB).  
- **JSON must share the same stem** as the image (`img_0001.tiff` â†” `img_0001.json`).  
- Supported JSON schemas:
  - LabelMe: `{"shapes":[{"points":[[x,y],...]},...]}`
  - Simple polygons: `{"polygons":[[[x,y],...],...],"height":H,"width":W}`
  - COCOâ€‘like: `{"annotations":[{"segmentation":[[x1,y1,x2,y2,...]]},...],"height":H,"width":W}`
- Any image **without** a JSON is **skipped** (warning printed), so training wonâ€™t crash.

---

## Configure the pipeline

Edit **`config.py`** to adjust everything in one place:

- `data_root`, `train_dir`, `val_dir`, `eval_dir`
- `checkpoint_dir` (`checkpoints/`) and `outputs_root` (`outputs/`)
- `image_size` `(W,H)` used during training & prediction
- `batch_size`, `epochs`, `lr`, `weight_decay`, `save_every`
- `pixel_threshold` for binarization
- `device`: `"auto"`, `"cuda"`, or `"cpu"`
- `resume_checkpoint`: name of a `.pth` file under `checkpoints/`

---

## Run the training & evaluation workflow

> Run all commands from the repository root with your virtualenv activated.

### 1. Train
```bash
python scripts/train.py
```
- Tracks **train/val loss, IoU, and Dice**.  
- Saves `checkpoints/model_best.pth` (by validation IoU).  
- To resume/fineâ€‘tune, set `resume_checkpoint` in `config.py`.

### 2. Evaluate
```bash
python scripts/eval.py
```
- Loads `model_best.pth` (or `resume_checkpoint`) and prints **IoU/Dice** on `data/eval` (skips samples without JSON).

### 3. Predict soft masks
```bash
python scripts/predict.py
```
- Runs on `data/eval/` and writes **probability maps** (`*_prob.png`) to `outputs/pred_masks/`.  
- Each map is resized back to the original image resolution.

### 4. Watershed postâ€‘processing
```bash
python scripts/postprocess.py
```
- Converts probability maps to **final binary masks** using a watershed refinement that splits merged regions and removes small artifacts.  
- Saves results to `outputs/postprocessed_masks/`.

---

## Outputs

```
checkpoints/
â””â”€â”€ model_best.pth

outputs/
â”œâ”€â”€ pred_masks/
â”‚   â””â”€â”€ img_0201_prob.png
â””â”€â”€ postprocessed_masks/
    â””â”€â”€ img_0201_mask.png
```

---

## Reproducibility tips

- Fix `seed` in `config.py` for deterministic runs.  
- Keep `image_size` and augmentations unchanged between train/val/eval.  
- Track dataset snapshots and Git commits for each experiment.

---

## Extending the project

- Swap the backbone: add new architectures under `unet_froth/models/`.  
- Add stronger augmentations in the dataset pipeline.  
- Integrate TensorBoard or Weights & Biases in `scripts/train.py`.  
- Add a slidingâ€‘window predictor for very large images.

---

## Troubleshooting

- **CUDA not available** â†’ set `device="cpu"` in `config.py`.  
- **Missing JSON** â†’ the dataset warns and skips the image.  
- **Masks too thin/thick** â†’ tune `pixel_threshold` (0.4â€“0.6 typical).  
- **Noisy instances** â†’ rely on `postprocess.py` (watershed), or increase `min_area` inside `unet_froth/utils/postprocess.py`.

---

## Authors & citation

- **Reza Dadbin** â€” <https://github.com/RezaDadbin>  
- **Sina Lotfi** â€” <https://github.com/cinaLotfi>

If you use this pipeline, please acknowledge:

> Reza Dadbin & Sina Lotfi â€” *UNetâ€‘based Froth Segmentation Pipeline*

---

**Note:** The overall structure and README flow are inspired by the SBSâ€‘Net froth segmentation pipeline (mirrored design: single config, scripts for train/eval/predict/postprocess).
